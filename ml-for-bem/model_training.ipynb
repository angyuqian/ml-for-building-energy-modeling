{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "v3zKYj2MQ84r"
      },
      "source": [
        "Based on *Using a deep temporal convolutional network as a building energy surrogate model that spans multiple climate zones*\n",
        "Recreation of Westermann et al., 2020\n",
        "\n",
        "https://doi.org/10.1016/j.apenergy.2020.115563\n",
        "\n",
        "https://gitlab.com/energyincities/building_surrogate_modelling"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HDMY9K9rQykv"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import matplotlib.pyplot as plt\n",
        "from tqdm.notebook import tqdm\n",
        "import h5py\n",
        "import os\n",
        "\n",
        "# torch.cuda.empty_cache()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### CUDA checks"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# print(f'PyTorch version: {torch.__version__}')\n",
        "# print('*'*10)\n",
        "# print(f'_CUDA version: ')\n",
        "# !nvcc --version\n",
        "# print('*'*10)\n",
        "# print(f'CUDNN version: {torch.backends.cudnn.version()}')\n",
        "# print(f'Available GPU devices: {torch.cuda.device_count()}')\n",
        "# print(f'Device Name: {torch.cuda.get_device_name()}')\n",
        "# print(torch.cuda.memory_summary())"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "IOdX7kH9ju29",
        "outputId": "6689ab4c-947c-4275-e7a0-46064aeb9fc8"
      },
      "outputs": [],
      "source": [
        "# is_cuda_runtime =  torch.cuda.is_available()\n",
        "# if not is_cuda_runtime:\n",
        "#   print(\"Change your runtime to a GPU-accelerated one.\")\n",
        "# else:\n",
        "#   print(\"All good!\")\n",
        "# device = \"cuda\" if is_cuda_runtime else \"cpu\"\\\n",
        "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "torch.cuda.memory_allocated()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 510
        },
        "id": "2NaxOkbKnki7",
        "outputId": "ad8ead8c-2898-4f1c-eb3c-cc99465ba356"
      },
      "outputs": [],
      "source": [
        "from storage import upload_to_bucket, download_from_bucket\n",
        "from schedules import mutate_timeseries\n",
        "from simulate import BatchSimulator\n",
        "from schema import Schema\n",
        "\n",
        "schema = Schema()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uS5TkYS8kG5-"
      },
      "outputs": [],
      "source": [
        "# !pip install vapeplot &> /dev/null"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "HgWx5pCLjOyg"
      },
      "outputs": [],
      "source": [
        "# @markdown ### 0.4 | Styling\n",
        "# import google\n",
        "# import vapeplot # https://github.com/dantaki/vapeplot\n",
        "import matplotlib\n",
        "# is_dark = google.colab.output.eval_js(\n",
        "#     'document.documentElement.matches(\"[theme=dark]\")'\n",
        "# )\n",
        "\n",
        "matplotlib.rcParams[\"figure.dpi\"] = 100\n",
        "matplotlib.rcParams[\"savefig.dpi\"] = 300\n",
        "# load style sheet for matplotlib, a plotting library we use for 2D visualizations\n",
        "plt.style.use(\n",
        "    \"https://github.com/dhaitz/matplotlib-stylesheets/raw/master/pitayasmoothie-dark.mplstyle\"\n",
        ")\n",
        "plt.style.use(\"dark_background\")\n",
        "plt.rcParams.update(\n",
        "    {\n",
        "        \"figure.facecolor\": (0.22, 0.22, 0.22, 1.0),\n",
        "        \"axes.facecolor\": (0.22, 0.22, 0.22, 1.0),\n",
        "        \"savefig.facecolor\": (0.22, 0.22, 0.22, 1.0),\n",
        "        \"grid.color\": (0.4, 0.4, 0.4, 1.0),\n",
        "    }\n",
        ")\n",
        "# vapeplot.set_palette(\"mallsoft\")\n",
        "plt.set_cmap(\"plasma\");"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "8hw2ET8mjT7e"
      },
      "source": [
        "# Download & preprocess the data"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "PM6j7S25RMD4"
      },
      "source": [
        "## Weather file & schedules preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7PZ1e7ikRPEM",
        "outputId": "94252bb5-a5b0-474b-c384-66bec0ca18c4"
      },
      "outputs": [],
      "source": [
        "# download_from_bucket(blob_name=f\"climate_array.npy\", file_name=\"./data/epws/climate_array.npy\")\n",
        "# download_from_bucket(blob_name=f\"tsol.npy\", file_name=\"./data/epws/tsol.npy\")\n",
        "# download_from_bucket(blob_name=f\"default_schedules.npy\", file_name=\"./data/template_libs/default_schedules.npy\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b_hxDCyzRUvU"
      },
      "outputs": [],
      "source": [
        "climate_array = np.load(\"./data/epws/climate_array.npy\")\n",
        "tsol_array = np.load(\"./data/epws/tsol.npy\")\n",
        "tsol_array = np.load(\"./data/epws/tsol.npy\")\n",
        "schedules = np.load(\"./data/template_libs/default_schedules.npy\") # 3x8760"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "oukelN_Qs9O7",
        "outputId": "fcd18427-e8ad-42a2-ac3f-a2cbc1acae73"
      },
      "outputs": [],
      "source": [
        "climate_array.shape"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ynBbI_keowzQ"
      },
      "source": [
        "Make a file in data called \"hdf5\""
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Normalization"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, let's set up the timeseries data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# TODO: normalize with the batch std? or recorded\n",
        "# nornalization method?\n",
        "# def normalize(data, mean, std):\n",
        "    # return (data-mean)/std\n",
        "def normalize(data, maxv, minv):\n",
        "    return (data-minv)/(maxv-minv)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "dbt_max = 50.0 # 50.5\n",
        "dbt_min = -35.0\n",
        "rh_max = 100.0\n",
        "rh_min = 0.0 # 2.0\n",
        "atm_max = 105800.0\n",
        "atm_min = 75600.0\n",
        "rad_min = 0.0\n",
        "ghrad_max = 1200.0 #1154.0\n",
        "dnrad_max = 1097.0\n",
        "dhrad_max = 689.0\n",
        "skyt_max = 32.3\n",
        "skyt_min = -58.3\n",
        "tsol_max = 60\n",
        "tsol_min = -40"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Normalize the whole climate and tsol array\n",
        "maxes = [dbt_max, rh_max, atm_max, ghrad_max, dnrad_max, dhrad_max, skyt_max]\n",
        "mins = [dbt_min, rh_min, atm_min, rad_min, rad_min, rad_min, skyt_min]\n",
        "\n",
        "norm_climate_array = np.zeros(climate_array.shape)\n",
        "norm_tsol_array = np.zeros(tsol_array.shape)\n",
        "for j in range(climate_array.shape[0]):\n",
        "    for i in range(7):\n",
        "        climate_array[j, i, :] = normalize(climate_array[j, i, :], maxes[i], mins[i])\n",
        "    for i in range(4):\n",
        "        norm_tsol_array[j, i, :] = normalize(tsol_array[j, i, :], tsol_max, tsol_min)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Schedules"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Now for the building vector"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "building_vect_schema = [\n",
        "    'width', \n",
        "    'height', \n",
        "    'facade_2_footprint', \n",
        "    'perim_2_footprint',\n",
        "    'roof_2_footprint',\n",
        "    'footprint_2_ground',\n",
        "    'wwr',\n",
        "    'orientation',\n",
        "    'HeatingSetpoint',\n",
        "    'CoolingSetpoint',\n",
        "    'LightingPowerDensity',\n",
        "    'EquipmentPowerDensity',\n",
        "    'PeopleDensity',\n",
        "    'Infiltration',\n",
        "    'FacadeMass',\n",
        "    'RoofMass',\n",
        "    'FacadeRValue',\n",
        "    'RoofRValue',\n",
        "    'SlabRValue',\n",
        "    ]\n",
        "\n",
        "building_vector_size = len(building_vect_schema) + 3 # (area/windowsettings)\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "And finally, the output energy data "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#TODO separate min and max for heating and cooling?\n",
        "# energy_min = 0\n",
        "# energy_max = 7 * 10**9\n",
        "\n",
        "# # Add core and perimeter together\n",
        "# heating = monthly[:, 0, :]+monthly[:, 2, :]\n",
        "# cooling = monthly[:, 1, :]+monthly[:, 3, :]\n",
        "# results = np.concatenate((np.expand_dims(heating, 1), np.expand_dims(cooling, 1)), axis=1)\n",
        "\n",
        "# norm_results = normalize(results, energy_max, energy_min)\n",
        "# print(monthly.shape)\n",
        "# ix = 17\n",
        "# plt.figure()\n",
        "# plt.plot(monthly[ix][0])\n",
        "# plt.plot(monthly[ix][1])\n",
        "# plt.plot(monthly[ix][2])\n",
        "# plt.plot(monthly[ix][3])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# for i in range(1000):\n",
        "#     plt.plot(norm_results[i, 0, :], 'orange', alpha=0.2)\n",
        "#     plt.plot(norm_results[i, 1, :], 'lightblue', alpha=0.5)\n",
        "# plt.plot(np.mean(norm_results[:, 0, :], axis=0), 'orangered', label=\"Heating\")\n",
        "# plt.plot(np.mean(norm_results[:, 1, :], axis=0), 'dodgerblue', label=\"Cooling\")\n",
        "# plt.legend()\n",
        "# plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Sample Loader from Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# the loading & preprocessing grouped into a function\n",
        "def load_training_samples(schema,\n",
        "    start_idx = 0,\n",
        "    count = 20000,\n",
        "    dbt_max = 50.0, # 50.5\n",
        "    dbt_min = -35.0,\n",
        "    rh_max = 100.0,\n",
        "    rh_min = 0.0, # 2.0\n",
        "    atm_max = 105800.0,\n",
        "    atm_min = 75600.0,\n",
        "    rad_min = 0.0,\n",
        "    ghrad_max = 1200.0, #1154.0\n",
        "    dnrad_max = 1097.0,\n",
        "    dhrad_max = 689.0,\n",
        "    skyt_max = 32.3,\n",
        "    skyt_min = -58.3,\n",
        "    tsol_max = 60,\n",
        "    tsol_min = -40,\n",
        "    a_max = 2000,\n",
        "    a_min = 0,\n",
        "    energy_min = 0,\n",
        "    energy_max = 30, # 30 kWh/m2/mo\n",
        "):\n",
        "    with h5py.File(f\"./data/model_data_manager/all_data_monthly.hdf5\", 'r') as f:\n",
        "        batch_size=count\n",
        "        monthly = f[\"monthly\"][start_idx:start_idx+batch_size] # this loads the whole batch into memory!\n",
        "        # total_heating = f[\"total_heating\"][0:batch_size] # this loads the whole batch into memory!\n",
        "        # total_cooling = f[\"total_cooling\"][0:batch_size] # this loads the whole batch into memory!\n",
        "        # errors = f[\"errors\"][...]\n",
        "        area = f[\"area\"][start_idx:start_idx+batch_size]\n",
        "        batch = f[\"storage_batch\"][start_idx:start_idx+batch_size]\n",
        "    # Normalize the whole climate and tsol array\n",
        "    maxes = [dbt_max, rh_max, atm_max, ghrad_max, dnrad_max, dhrad_max, skyt_max]\n",
        "    mins = [dbt_min, rh_min, atm_min, rad_min, rad_min, rad_min, skyt_min]\n",
        "\n",
        "    norm_climate_array = np.zeros(climate_array.shape)\n",
        "    norm_tsol_array = np.zeros(tsol_array.shape)\n",
        "    for j in range(climate_array.shape[0]):\n",
        "        for i in range(7):\n",
        "            climate_array[j, i, :] = normalize(climate_array[j, i, :], maxes[i], mins[i])\n",
        "        for i in range(4):\n",
        "            norm_tsol_array[j, i, :] = normalize(tsol_array[j, i, :], tsol_max, tsol_min)\n",
        "\n",
        "    # Schedules\n",
        "    seeds = schema[\"schedules_seed\"].extract_storage_values_batch(batch).astype(int) # 1x1\n",
        "    operations_maps = schema[\"schedules\"].extract_storage_values_batch(batch) # 3\n",
        "\n",
        "    # timeseries_vector = np.zeros((data_size, 8, 8760))\n",
        "    data_size = batch.shape[0]\n",
        "    timeseries_vector = np.zeros((data_size, 11, 8760))\n",
        "    orientations = schema['orientation'].extract_storage_values_batch(batch).flatten()\n",
        "    idxs = schema['base_epw'].extract_storage_values_batch(batch).flatten()\n",
        "    # Get the weather file for each entry\n",
        "    for i, epw_idx in enumerate(idxs):\n",
        "        epw_idx = int(epw_idx)\n",
        "        timeseries_vector[i, :-4, :] = norm_climate_array[epw_idx]\n",
        "        timeseries_vector[i, -4:-1, :] = mutate_timeseries(schedules, operations_maps[i], seeds[i])\n",
        "        timeseries_vector[i, -1, :] = norm_tsol_array[epw_idx, int(orientations[i]), :]\n",
        "    wind_settings = schema[\"WindowSettings\"].extract_storage_values_batch(batch)\n",
        "\n",
        "    \n",
        "\n",
        "    building_vector = np.zeros((data_size, 22))\n",
        "    for i, schema_name in enumerate(building_vect_schema):\n",
        "        column = schema[schema_name].extract_storage_values_batch(batch).flatten()\n",
        "        try:\n",
        "            column = normalize(column, schema[schema_name].max, schema[schema_name].min)\n",
        "        except:\n",
        "            print(f\"Calculating mean from batch data for {schema_name}...\")\n",
        "            column = normalize(column, np.max(column), np.min(column))\n",
        "        building_vector[:, i] = column\n",
        "    building_vector[:, -3] = normalize(area, a_max, a_min)\n",
        "    building_vector[:, -2] = normalize(wind_settings[:, 0].flatten(), schema[\"WindowSettings\"].max[0], schema[\"WindowSettings\"].min[0]) # U-val\n",
        "    building_vector[:, -1] = normalize(wind_settings[:, 1].flatten(), schema[\"WindowSettings\"].max[1], schema[\"WindowSettings\"].min[1]) # shgc\n",
        "\n",
        "    # Add core and perimeter together\n",
        "    heating = (monthly[:, 0, :]+monthly[:, 2, :]) / area.reshape(-1,1)\n",
        "    cooling = (monthly[:, 1, :]+monthly[:, 3, :]) / area.reshape(-1,1)\n",
        "    area_normed_results = np.concatenate((np.expand_dims(heating, 1), np.expand_dims(cooling, 1)), axis=1)\n",
        "\n",
        "    norm_results = normalize(area_normed_results, energy_max*2.777e7, energy_min*2.777e7)\n",
        "    # TODO: MAKE THIS EUI?\n",
        "\n",
        "    return building_vector, timeseries_vector, norm_results"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Load Part of the Dataset into Memory"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "%%time\n",
        "start_idx = 0\n",
        "bldg_count = 3000\n",
        "\n",
        "building_vector, timeseries_vector, norm_results = load_training_samples(schema, start_idx=start_idx, count=bldg_count)\n",
        "\n",
        "for i in range(bldg_count):\n",
        "    plt.plot(norm_results[i, 0, :], 'orange', alpha=0.2)\n",
        "    plt.plot(norm_results[i, 1, :], 'lightblue', alpha=0.5)\n",
        "plt.plot(np.mean(norm_results[:, 0, :], axis=0), 'orangered', label=\"Heating\")\n",
        "plt.plot(np.mean(norm_results[:, 1, :], axis=0), 'dodgerblue', label=\"Cooling\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "for c in range(len(building_vect_schema) + 3):\n",
        "    y = building_vector[:, c]\n",
        "    # Add some random \"jitter\" to the x-axis\n",
        "    x = np.random.normal(c, 0.01, size=len(y))\n",
        "    plt.plot(x, y, '.', alpha=0.1)\n",
        "plt.boxplot(building_vector)\n",
        "plt.xticks(ticks = list(range(22)), labels=building_vect_schema + [\"area\", \"U-val\", \"SHGC\"], rotation = 90)\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "6kIlpcnvjYPH"
      },
      "source": [
        "# Set up the model"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "SinGWuvSRUKU"
      },
      "source": [
        "## Timeseries CNN"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "93X0V4EuRm1S"
      },
      "outputs": [],
      "source": [
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "\n",
        "class EnergyTimeseriesCNNBlockA(nn.Module):\n",
        "    def __init__(self, \n",
        "                 in_channels=11,\n",
        "                 n_feature_maps = 64,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_feature_maps=n_feature_maps\n",
        "\n",
        "        self.input_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=n_feature_maps,\n",
        "                kernel_size=8,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(n_feature_maps),\n",
        "        )\n",
        "\n",
        "        self.mid_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=n_feature_maps,\n",
        "                out_channels=n_feature_maps,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(n_feature_maps),\n",
        "        )\n",
        "\n",
        "        self.final_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=n_feature_maps,\n",
        "                out_channels=n_feature_maps,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(n_feature_maps),\n",
        "        )\n",
        "\n",
        "        self.skip_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=n_feature_maps,\n",
        "                kernel_size=1,\n",
        "                stride=1,\n",
        "                padding=0,\n",
        "            ),\n",
        "            nn.BatchNorm1d(n_feature_maps), \n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_skip = self.skip_layer(x)\n",
        "        \n",
        "        x_out = self.input_convolutional_layer(x)\n",
        "        x_out = nn.functional.relu(x_out)\n",
        "\n",
        "        x_out = self.mid_convolutional_layer(x_out)\n",
        "        x_out = nn.functional.relu(x_out)\n",
        "\n",
        "        x_out = self.final_convolutional_layer(x_out)\n",
        "        \n",
        "        x_out = x_out + x_skip\n",
        "        \n",
        "        return nn.functional.relu(x_out)\n",
        "\n",
        "class EnergyTimeseriesCNNBlockB(nn.Module):\n",
        "    def __init__(self, \n",
        "                 in_channels=128,\n",
        "                 n_feature_maps = 128,\n",
        "                 ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.n_feature_maps=n_feature_maps\n",
        "\n",
        "        self.input_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=n_feature_maps,\n",
        "                kernel_size=8,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(n_feature_maps),\n",
        "        )\n",
        "\n",
        "        self.mid_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=n_feature_maps,\n",
        "                out_channels=n_feature_maps,\n",
        "                kernel_size=5,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(n_feature_maps),\n",
        "        )\n",
        "\n",
        "        self.final_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=n_feature_maps,\n",
        "                out_channels=n_feature_maps,\n",
        "                kernel_size=3,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(n_feature_maps),\n",
        "        )\n",
        "\n",
        "        self.skip_layer = nn.BatchNorm1d(n_feature_maps)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x_skip = self.skip_layer(x)\n",
        "        \n",
        "        x_out = self.input_convolutional_layer(x)\n",
        "        x_out = nn.functional.relu(x_out)\n",
        "\n",
        "        x_out = self.mid_convolutional_layer(x_out)\n",
        "        x_out = nn.functional.relu(x_out)\n",
        "\n",
        "        x_out = self.final_convolutional_layer(x_out)\n",
        "        \n",
        "        x_out = x_out + x_skip\n",
        "        \n",
        "        return nn.functional.relu(x_out)\n",
        "\n",
        "\n",
        "class AnnualEnergyCNN(nn.Module):\n",
        "    def __init__(self, \n",
        "               out_channels=22,\n",
        "               n_feature_maps = 64,\n",
        "               ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resblock_1 = EnergyTimeseriesCNNBlockA(\n",
        "            n_feature_maps=n_feature_maps\n",
        "            )\n",
        "        \n",
        "        self.resblock_2 = EnergyTimeseriesCNNBlockA(\n",
        "            in_channels=n_feature_maps,\n",
        "            n_feature_maps=n_feature_maps*2\n",
        "            )\n",
        "        \n",
        "        # no need to expand channels in third layer because they are equal\n",
        "        self.resblock_3 = EnergyTimeseriesCNNBlockB(\n",
        "            in_channels=n_feature_maps*2,\n",
        "            n_feature_maps=n_feature_maps*2\n",
        "            )\n",
        "        \n",
        "        # FOR ANNUAL\n",
        "        self.GlobalAveragePool = nn.AvgPool1d(kernel_size=8760) # 1D? average across all feature maps\n",
        "        self.linear = nn.Linear(in_features=n_feature_maps*2, out_features=out_channels)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resblock_1(x)\n",
        "        x = self.resblock_2(x)\n",
        "        x = self.resblock_3(x)\n",
        "        x = self.GlobalAveragePool(x)\n",
        "        x = x.squeeze(-1)\n",
        "        x = self.linear(x)\n",
        "        return nn.functional.relu(x)\n",
        "\n",
        "class MonthlyEnergyCNN(nn.Module):\n",
        "    def __init__(self, \n",
        "               in_channels=8,\n",
        "               out_channels=8,\n",
        "               n_feature_maps = 64,\n",
        "               ):\n",
        "        super().__init__()\n",
        "\n",
        "        self.resblock_1 = EnergyTimeseriesCNNBlockA(\n",
        "            n_feature_maps=n_feature_maps\n",
        "            )\n",
        "        \n",
        "        self.resblock_2 = EnergyTimeseriesCNNBlockA(\n",
        "            in_channels=n_feature_maps,\n",
        "            n_feature_maps=n_feature_maps*2\n",
        "            )\n",
        "        \n",
        "        # no need to expand channels in third layer because they are equal\n",
        "        self.resblock_3 = EnergyTimeseriesCNNBlockB(\n",
        "            in_channels=n_feature_maps*2,\n",
        "            n_feature_maps=n_feature_maps*2\n",
        "            )\n",
        "        \n",
        "        # FOR MONTHLY (out is 2x12)\n",
        "        self.month_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=n_feature_maps*2,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=30,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "        )   \n",
        "        self.pooling = nn.AvgPool1d(kernel_size=730)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.resblock_1(x)\n",
        "        x = self.resblock_2(x)\n",
        "        x = self.resblock_3(x)\n",
        "        x = self.pooling(x)\n",
        "        x = self.month_convolutional_layer(x)\n",
        "        return nn.functional.relu(x)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "chI5XS9H8vwj"
      },
      "source": [
        "## Energy surrogate"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "BIKoTzz-8zCJ"
      },
      "outputs": [],
      "source": [
        "class EnergyCNN(torch.nn.Module):\n",
        "    def __init__(self, \n",
        "                 in_channels=30,\n",
        "                 n_feature_maps=64,\n",
        "                 out_channels=2\n",
        "                 ):\n",
        "        super(EnergyCNN, self).__init__()\n",
        "\n",
        "        # FOR MONTHLY (out is 2x12)\n",
        "        self.in_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=in_channels,\n",
        "                out_channels=n_feature_maps,\n",
        "                kernel_size=2,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(n_feature_maps),\n",
        "            )\n",
        "        \n",
        "        self.out_convolutional_layer = nn.Sequential(\n",
        "            nn.Conv1d(\n",
        "                in_channels=n_feature_maps,\n",
        "                out_channels=out_channels,\n",
        "                kernel_size=2,\n",
        "                stride=1,\n",
        "                padding='same',\n",
        "            ),\n",
        "            nn.BatchNorm1d(out_channels),\n",
        "            )  \n",
        "        # self.pooling = nn.AvgPool1d(kernel_size=730)\n",
        "\n",
        "    def forward(self, sample):\n",
        "        # sample (22+n, 1)\n",
        "        x = self.in_convolutional_layer(sample)\n",
        "        x = nn.functional.leaky_relu(x)\n",
        "        x = self.out_convolutional_layer(x)\n",
        "        x = nn.functional.leaky_relu(x)\n",
        "\n",
        "        return x"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Init Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2QgqLjIIIUB2"
      },
      "outputs": [],
      "source": [
        "latent_size = 22\n",
        "building_vector_size = len(building_vect_schema)+3\n",
        "energy_input_size = latent_size + building_vector_size\n",
        "timeseries_model = MonthlyEnergyCNN(out_channels=latent_size).to(device)\n",
        "energy_model = EnergyCNN(in_channels=energy_input_size).to(device)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "-3PN3seJjfZ0"
      },
      "source": [
        "# Dataloaders"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "First, split the data into training, validation, and testing (80/10/10)."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "def make_dataset(start_idx, count, loader_batch_size=100):\n",
        "    torch.cuda.empty_cache()\n",
        "    building_vector, timeseries_vector, norm_results = load_training_samples(schema, start_idx=start_idx, count=count)\n",
        "    dataset  = {}\n",
        "    for i in range(building_vector.shape[0]):\n",
        "        # DICT ENTRIES MUST BE IN ORDER\n",
        "        dataset[i] = dict({\n",
        "            \"building_vector\": np.array([building_vector[i]]*12).T,\n",
        "            \"timeseries_vector\": timeseries_vector[i],\n",
        "            \"results_vector\": norm_results[i],\n",
        "        })\n",
        "    generator = torch.Generator()\n",
        "    generator.manual_seed(0)\n",
        "\n",
        "    train, val, test = torch.utils.data.random_split(dataset, lengths=[0.8, 0.1, 0.1], generator=generator)\n",
        "    training_dataloader = torch.utils.data.DataLoader(train, batch_size=loader_batch_size, shuffle=False)\n",
        "    validation_dataloader = torch.utils.data.DataLoader(val, batch_size=loader_batch_size, shuffle=False)\n",
        "    test_dataloader = torch.utils.data.DataLoader(test, batch_size=loader_batch_size, shuffle=False)\n",
        "    return {\"datasets\": {\"train\": train, \"test\": test, \"validate\": val}, \"dataloaders\": {\"train\": training_dataloader, \"test\": test_dataloader, \"validate\": validation_dataloader}}\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "iyYProobCseL"
      },
      "source": [
        "# Train the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wEFuOzCD8jA_"
      },
      "outputs": [],
      "source": [
        "timeseries_model.train()\n",
        "energy_model.train()\n",
        "\n",
        "n_full_epochs = 3\n",
        "n_mini_epochs = 5\n",
        "mini_epoch_batch_size = 20000\n",
        "available_simulations = 100000 - mini_epoch_batch_size\n",
        "loader_batch_size = 100\n",
        "\n",
        "learning_rate = 1e-4\n",
        "step_loss_frequency = 50\n",
        "training_loss_history = []\n",
        "validation_loss_history  = []\n",
        "latentvect_history  = []\n",
        "\n",
        "# LOSS FUNCTION\n",
        "loss_function = nn.MSELoss()\n",
        "\n",
        "# optimizer = torch.optim.Adam(energy_model.parameters(), lr=learning_rate)\n",
        "optimizer = torch.optim.Adam(list(timeseries_model.parameters()) + list(energy_model.parameters()), lr=learning_rate)\n",
        "\n",
        "for start_idx in range(0,available_simulations,mini_epoch_batch_size):\n",
        "    print(f\"{'-'*20} Epoch {'-'*20}\")\n",
        "    print(f\"Simulations: {start_idx} to {start_idx+mini_epoch_batch_size}\")\n",
        "    print(f\"Loading data...\")\n",
        "    data = make_dataset(start_idx=start_idx, count=mini_epoch_batch_size, loader_batch_size=loader_batch_size)\n",
        "    training_dataloader = data[\"dataloaders\"][\"train\"]\n",
        "    validation_dataloader = data[\"dataloaders\"][\"validate\"]\n",
        "\n",
        "    print(\"Starting training...\")\n",
        "\n",
        "    for epoch_num in range(n_mini_epochs):\n",
        "        print(f\"{'-'*20} MiniBatch Epoch number {epoch_num} {'-'*20}\")\n",
        "        for j, sample in enumerate(training_dataloader):\n",
        "            optimizer.zero_grad()\n",
        "            \n",
        "            timeseries = sample[\"timeseries_vector\"].to(device).float()\n",
        "            bldg_vect = sample[\"building_vector\"].to(device).float()\n",
        "            # Repeat bldg_vect for monthly size:\n",
        "            loads = sample[\"results_vector\"].to(device).float()\n",
        "\n",
        "            timeseries_latvect = timeseries_model(timeseries)\n",
        "\n",
        "            # Concatenate vectors\n",
        "            x = torch.cat([timeseries_latvect, bldg_vect], axis=1).squeeze(1)\n",
        "            # print(x.shape)\n",
        "\n",
        "            predicted_loads = energy_model(x)\n",
        "\n",
        "            loss = loss_function(loads, predicted_loads)\n",
        "\n",
        "            if j%step_loss_frequency == 0:\n",
        "                print(f\"Step {j} loss: {loss.item()}\")\n",
        "                latentvect_history.append(timeseries_latvect.detach())\n",
        "            \n",
        "            training_loss_history.append([len(training_loss_history),loss.item()])\n",
        "\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "\n",
        "        with torch.no_grad():\n",
        "            epoch_validation_loss = []\n",
        "            for sample in validation_dataloader:\n",
        "                # SET UP SAMPLE DATA\n",
        "                timeseries_val = sample[\"timeseries_vector\"].to(device).float()\n",
        "                # print(timeseries.shape)\n",
        "                bldg_vect_val = sample[\"building_vector\"].to(device).float()\n",
        "                # Repeat bldg_vect for monthly size:\n",
        "                # bldg_vect_month_val = np.repeat(bldg_vect_val.unsqueeze(2), 12, axis=2)\n",
        "                loads = sample[\"results_vector\"].to(device).float()\n",
        "\n",
        "                timeseries_latvect_val = timeseries_model(timeseries_val)\n",
        "\n",
        "                # Concatenate vectors\n",
        "                x_val = torch.cat([timeseries_latvect_val, bldg_vect_val], axis=1).squeeze(1)\n",
        "\n",
        "                predicted_loads = energy_model(x_val)\n",
        "\n",
        "                loss = loss_function(loads, predicted_loads)\n",
        "                \n",
        "                epoch_validation_loss.append(loss.item())\n",
        "\n",
        "            mean_validation_loss = np.mean(epoch_validation_loss)\n",
        "            print(f\"Mean validation loss for batch: {mean_validation_loss}\")\n",
        "\n",
        "            validation_loss_history.append([len(training_loss_history), mean_validation_loss])\n",
        "    del training_dataloader\n",
        "    del validation_dataloader\n",
        "    del data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save checkpoint\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d%H%M\")\n",
        "uid = 'SW'\n",
        "torch.save({\n",
        "    'epoch': epoch_num,\n",
        "    'model_state_dict': timeseries_model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss.item(),\n",
        "    },\n",
        "    f\"./checkpoints/{timestamp}_timeseries_{uid}.pt\"\n",
        ")\n",
        "torch.save({\n",
        "    'epoch': epoch_num,\n",
        "    'model_state_dict': energy_model.state_dict(),\n",
        "    'optimizer_state_dict': optimizer.state_dict(),\n",
        "    'loss': loss.item(),\n",
        "    },\n",
        "    f\"./checkpoints/{timestamp}_energy_{uid}.pt\"\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# do a visualization of data after each block?\n",
        "bldgs_to_plot = 10\n",
        "fig, ax = plt.subplots(ncols=10, nrows=1, figsize=(20,10))\n",
        "for i in range(bldgs_to_plot):\n",
        "    ax[i].imshow(timeseries_latvect[int(np.random.rand()*timeseries_latvect.shape[0])].detach().cpu().numpy())\n",
        "    # ax[i].set_axis(\"off\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "ppRAOa-bi7Vq"
      },
      "source": [
        "Plot the training and validation loss and history"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YlwfhPYei_HD"
      },
      "outputs": [],
      "source": [
        "training_loss_history_array = np.array(training_loss_history)\n",
        "validation_loss_history_array = np.array(validation_loss_history)\n",
        "\n",
        "plt.figure(figsize=(6,3))\n",
        "plt.plot(training_loss_history_array[:,0],training_loss_history_array[:,1], label=\"Training loss\")\n",
        "plt.plot(validation_loss_history_array[:,0],validation_loss_history_array[:,1], label=\"Validation loss\")\n",
        "plt.legend()\n",
        "plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "n_lT7mMVeWjW"
      },
      "source": [
        "# Testing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "timeseries_model.eval()\n",
        "energy_model.eval()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "all_loads = []\n",
        "all_results = []\n",
        "unseen_climates = make_dataset(start_idx=150000, count=3000)\n",
        "test_dataloader = unseen_climates[\"dataloaders\"][\"test\"]\n",
        "with torch.no_grad():\n",
        "    for test_samples in test_dataloader:\n",
        "        test_samples = next(iter(test_dataloader))\n",
        "        timeseries = test_samples[\"timeseries_vector\"].to(device).float()\n",
        "        bldg_vect = test_samples[\"building_vector\"].to(device).float()\n",
        "        loads = test_samples[\"results_vector\"].to(device).float()\n",
        "\n",
        "        # print(\"bldg_vect: \", bldg_vect.shape)\n",
        "        # print(\"timeseries: \", timeseries.shape)\n",
        "        # print(\"loads: \", loads.shape)\n",
        "        timeseries_latvect = timeseries_model(timeseries)\n",
        "        # print(\"latent: \", timeseries_latvect.shape)\n",
        "\n",
        "        # Concatenate vectors\n",
        "        x = torch.cat([timeseries_latvect, bldg_vect], axis=1)\n",
        "        x = x.squeeze(1)\n",
        "        # print(\"energy input: \", x.shape)\n",
        "\n",
        "        predicted_loads = energy_model(x)\n",
        "        all_loads.append(loads)\n",
        "        all_results.append(predicted_loads)\n",
        "\n",
        "        loss = loss_function(loads, predicted_loads)\n",
        "\n",
        "all_loads = torch.vstack(all_loads)\n",
        "all_results = torch.vstack(all_results)\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i = 200\n",
        "for i in range(10):\n",
        "    plt.figure()\n",
        "    plt.plot(all_loads[i, 0, :].cpu(), \"-o\",label=\"Predicted heating\")\n",
        "    plt.plot(all_loads[i, 1, :].cpu(), \"-o\",label=\"Predicted cooling\")\n",
        "    plt.plot(all_results[i, 0, :].cpu(), label=\"Actual heating\")\n",
        "    plt.plot(all_results[i, 1, :].cpu(), label=\"Actual cooling\")\n",
        "    plt.ylim([-0.01,0.5])\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# i = 200\n",
        "for i in range(10):\n",
        "    plt.figure()\n",
        "    plt.plot(all_loads[i, 0, :].cpu(), \"-o\",label=\"Predicted heating\")\n",
        "    plt.plot(all_loads[i, 1, :].cpu(), \"-o\",label=\"Predicted cooling\")\n",
        "    plt.plot(all_results[i, 0, :].cpu(), label=\"Actual heating\")\n",
        "    plt.plot(all_results[i, 1, :].cpu(), label=\"Actual cooling\")\n",
        "    plt.ylim([-0.01,0.5])\n",
        "    plt.legend()\n",
        "    plt.show()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "metadata": {
        "id": "VMZu6SQLmEu8"
      },
      "source": [
        "# Visualizations\n",
        "- Latent space - T-sne\n",
        "- "
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.16"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
